{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6d3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/alexnguyen9/recipe-matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3edb9c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n"
     ]
    }
   ],
   "source": [
    "print(\"connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "856e288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7556d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d63f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67a3475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='halifaxfoodie-ml-data'\n",
    "\n",
    "allrecipes_recipes = 'allrecipes-recipes.json'\n",
    "allrecipes_recipesJson = 's3://{}/{}'.format(bucket, allrecipes_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc6fffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbccouk_recipes = 'bbccouk-recipes.json'\n",
    "bbccouk_recipesJson = 's3://{}/{}'.format(bucket, bbccouk_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f0f99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epicurious_recipes = 'epicurious-recipes.json'\n",
    "epicurious_recipesJson = 's3://{}/{}'.format(bucket, epicurious_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c95b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading JSON Files...\n",
      "Cleaning ingredients...\n",
      "Vectorizing ingredients...\n",
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def singular(x):\n",
    "    return [wnl.lemmatize(s) for s in x]\n",
    "\n",
    "# get only nouns and adjectives (for some reason it kept removing 'pepper' and 'chicken')\n",
    "def get_nouns_and_adj(l):\n",
    "    return [m[0] for m in itertools.chain.from_iterable([nltk.pos_tag(nltk.word_tokenize(x)) for x in l]) if (m[1] in ['NN','JJ','NNS','NNP','NNPS'] or m[0] in ['pepper','chicken'])]\n",
    "\n",
    "# change certain compound words to singular words\n",
    "\n",
    "# remove meausure words, preparation adjectives\n",
    "def remove_words(x):\n",
    "    remove = ['quart','liter','ml','teaspoon','pt','tablespoon','cup','ounce','fluid','gallon','pint',\n",
    "          'pound','slice','sheet','pound','gram','ml','stick','bulb','inch','pinch','large',\n",
    "         'small','light','sprig','quarter','half','whole','handful','good','best','fresh',\n",
    "         'package','can','packed','stem','medium','piece','stalk','finishing','bottle','container',\n",
    "         'clove','ear','fine','quality','coarse','bunch','wedge','flat','ground','lb','c','tbs',\n",
    "         'thin','wide','refridgerator','equipment','standard','b','unprocessed','en', 'round',\n",
    "          'optional','tsp','warm','cold','chopped','boiling','kitchen','length','lengthwise','smallish',\n",
    "          'quick','dry','wet','new','few','many','splash','drop','topping','pure','regular','oz',\n",
    "          'jar','envelope','extra','generous','hard','old','little','different','low','fat','gluten','free',\n",
    "          'raw','square','foil','special','store','hard','soft','frozen','bag','recipe','decadent','spiral',\n",
    "          'mini','simple','cooked','dark','packet','pre','box','unsalted','firm','other','tb','thread',\n",
    "          'strand','strip','thick','restaurant','accompaniment','kg','lbs','ripe','boneless','range','zesty',\n",
    "          'sodium','lowfat','original','tbsp','fl','peel','available','dash','nonstick','adjustable',\n",
    "          'natural','zest','preheat','head','refridgerated','such','uncooked','canned','size','skinless',\n",
    "          'frying','baby','size','artisan','organic','canned','sliced','cooled','chilled','part',\n",
    "          'peeled','bottled','unpeeled','crunchy','pt','litre','additional','addition','wrapped','sweetened']\n",
    "    return [y for y in x if y not in remove]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"Reading JSON Files...\")\n",
    "    data_bbc = pd.read_json(bbccouk_recipesJson,lines=True)\n",
    "    #data_cookstr = pd.read_json(\"data/cookstr-recipes.json\",lines=True)\n",
    "    data_epi = pd.read_json(epicurious_recipesJson,lines=True)\n",
    "    data_ar = pd.read_json(allrecipes_recipesJson,lines=True)\n",
    "\n",
    "    # fix epicurious food dataframe\n",
    "    data_epi.rename(columns = {'hed':'title','prepSteps':'instructions'},inplace=True)\n",
    "    data_epi.url = data_epi.url.apply(lambda x: 'www.epicurious.com' + x)\n",
    "\n",
    "    # combine all the food recipes and get only the relevant columns (title, ingredients, instructions, url)\n",
    "    combined = pd.concat([data_bbc,data_epi,data_ar],join='inner',ignore_index=True)\n",
    "\n",
    "\n",
    "    # remove this food item since I think there is a bug in the scraper\n",
    "    combined = combined[combined.title != 'Johnsonville® Three Cheese Italian Style Chicken Sausage Skillet Pizza']\n",
    "    combined.dropna(inplace=True)\n",
    "\n",
    "    # get recipes with at least 3 or more ingredients\n",
    "    combined = combined[combined.ingredients.apply(len) > 2]\n",
    "\n",
    "    # turn the list into a string\n",
    "    combined.instructions = combined.instructions.apply(lambda x:' '.join(x))\n",
    "\n",
    "    # reset index\n",
    "    combined.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    print(\"Cleaning ingredients...\")\n",
    "    # clean the recipes\n",
    "    recipe = combined.ingredients.apply(lambda x: [re.sub(\",.*$\", \"\", y).lower() for y in x]) # remove everything after a comma and make lower case\n",
    "    recipe = recipe.apply(lambda x: [re.sub('é','e', y)  for y in x]) # change accented 'e'\n",
    "    recipe = recipe.apply(lambda x: [re.sub('î','i', y)  for y in x]) # change accented 'i'\n",
    "    recipe = recipe.apply(lambda x: [re.sub(r'[^\\x00-\\x7f]',r' ', y)  for y in x]) # remove accented characters\n",
    "    recipe = recipe.apply(lambda x: [re.sub(\" with.*$\", \"\", y) for y in x]) # everything after a 'with'\n",
    "    recipe = recipe.apply(lambda x: [re.sub('\\([^()]*\\)', \"\", y) for y in x]) # everything in parenthesis\n",
    "    recipe = recipe.apply(lambda x: [re.sub(r'\\W+',\" \", y) for y in x]) # only alphanumeric characters\n",
    "\n",
    "    recipe = recipe.apply(get_nouns_and_adj) # get only nouns and adjectives\n",
    "    recipe = recipe.apply(singular) # convert to singular words\n",
    "    recipe = recipe.apply(remove_words) # remove irrelatvent words\n",
    "\n",
    "    print(\"Vectorizing ingredients...\")\n",
    "    # define the count vectorizer\n",
    "    vc = CountVectorizer(stop_words='english',min_df=60,binary=True)\n",
    "\n",
    "    # this is the document term matrix from our recipes\n",
    "    X = vc.fit_transform(recipe.values)\n",
    "\n",
    "    print(\"Pickling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bd56cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X.toarray().astype(bool), open('food_matrix.pkl','wb')) # save as boolean array to save space\n",
    "pickle.dump(combined, open('main_data.pkl','wb'))\n",
    "pickle.dump(vc, open('transformer.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
